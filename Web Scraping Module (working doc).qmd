---
title: "An Introduction to Web Scraping With Soccer News"
author: "Janelyn Geronimo"
toc: true
format:
  html:
    embed-resources: true
---

# Introduction

## Soccer, Transfers, and Transfermarkt

Soccer, the most popular sport worldwide, is also very well known for its player transfer system. To learn more about what transfers are and how they work, you can watch this short video: {{< video https://www.youtube.com/watch?v=tJLsYljtdaI aspect-ratio="21x9" >}}

Because player transfers are very lucrative, fans and coaches alike have taken to tracking and speculating player valuations and contracts. Transfermarkt is one of many soccer news websites that tracks player and team stats, player valuations, as well as news and rumors about transfers. In this module, you will learn how to web scrape articles from Transfermarkt to later do text analysis on transfer news.

## What is Web Scraping? How is it Related to Soccer?

Web scraping is the automated process of collecting data from a website. It is a very useful technique that can be used to create datasets from website information. While there are many types of data that can be scraped from websites, this module focuses primarily on collecting text data.

But, you might ask, what is the point of collecting text data from soccer articles? One reason that this module focuses on text harvesting is that there are no (widely) available text datasets for soccer online, whereas there are many quantitative libraries and datasets available for analysis. So, if you are a soccer fan and wanted to perform a text analysis on players being transferred, you would not have the luxury of accessing an existing text dataset. It is in situations like these when there is no available data, yet lots of freely available information on websites that web scraping shines: by using web scraping, you can automate data collection, choose the information that is being collected, and forego the need of relying on existing sources for data. Therefore, while it is not a necessary tool for data analysts and scientists, learning how to web scrape can prove to be very helpful for data collection and investigation.

## Ethics of Web Scraping

While there are certainly many applications of web scraping, it is also important to address the ethical concerns that come with relying on this data collection method. Although some websites allow web scraping, others strictly prohibit the practice. Therefore, in order to ethically web scrape, it is good practice to check if a website allows bots to scrape information. You can check if web scraping is allowed on a website by checking the robots.txt file of the site. The robots.txt file for a site, if it exists, can be accessed by taking the homepage URL of the site and adding /robots.txt to the end of it. Since we will be scraping articles from Transfermarkt, we can use the site as an example of a page that allows scraping. By going to [transfermarkt.com/robots.txt](transfermarkt.com/robots.txt), we see the following page:![](robots_txt_img.png)

We can see that while certain bots are not allowed to access Transfermarkt, `Slurp` and `*` are allowed to access Transfermarkt, as indicated by the `Allow: /` string. Since the `*` is meant to represent bots not disallowed from using the website, this means that we are fine to scrape on Transfermarkt. All websites have different rules so it is always best to check first before web scraping. To read a little more about how to interpret robots.txt, you can read more about it in [this](https://www.cloudflare.com/learning/bots/what-is-robots-txt/) article by Cloudflare. You can also find more information about web scraping and ethics in Elmore and Urbaczewski (2025).

## Learning Objectives

By the end of this activity, students will learn:

-   basic HTML

-   how to identify HTML elements using different tools (SelectorGadget and/or Inspect Element)

-   the differences between HTML elements and attributes and how to extract them

-   how to turn semi-structured data (raw HTML) into structured data (clean dataframes) for text mining

-   how to automate web scraping

# Methods

-   rvest for web scraping, tibble for data frame creation

# Getting Started

## Setup

To get started, let's install and load the library rvest.

```{r}
# install.packages("rvest")
library(rvest)
```

This library is used primarily to collect text information from websites, and in this module will only be applied to articles in Transfermarkt. To begin, let's start by taking a random article from Tranfermarkt. The link to the page that we will be scraping is here:

<https://www.transfermarkt.com/flops-or-smart-deals-analysing-the-premier-leagues-10-biggest-signings-from-the-bundesliga/view/news/458399>

## How Web Scraping Works

When you open the page in your web browser, it should look like this: ![](transfermarkt_page_image.png)While there may be nothing special about how the web page looks to us, the way that a computer reads the website is much different from how we read it. Although a user of this website may only care for the contents of the article itself, when the page is scraped we get all of the elements that are displayed on the website. That includes (but is not limited to) the article's text, the profile information on the sidebars, and team information. These items are all encased in the website's HTML, which is what we will be scraping and turning into structured data. To extract the HTML from the page we run the following code:

```{r}
page_url <- "https://www.transfermarkt.com/flops-or-smart-deals-analysing-the-premier-leagues-10-biggest-signings-from-the-bundesliga/view/news/458399"
page_html <- read_html(page_url)
```

Now, if we call the `page_html` object, we can see that the returned item is not just the article, but all of the HTML elements that structure the webpage. This is the raw structure of the website, and R returns this information as a list.

::: {.callout-tip title="Term Glossary"}
**HTML:** An abbreviation for **H**yper**t**ext **M**arkup **L**anguage, HTML is a computer language that dictates how a computer will display a web page. HTML provides structure for web pages and is used on every website you can access online.

**HTML Element:** An element in HTML consists of tags which decide how text is displayed. HTML element tags encase text or other information and are always structured with a beginning and end. For example take the line of text displayed below:

\<p\>Hello!\</p\>

The above line is an HTML element with the `<p>` tag. An important thing to notice here is that the beginning of the tag only has the `p` and the end of the tag includes the forward slash `/p`. This notation allows for the computer to know where that element starts and ends, which means that elements of different tags can be encased in each other.

**HTML Attribute:** An attribute is information included alongside an element. You can think of it as an element within another element which can contain information such as links or text that appears when hovered over.
:::

```{r}
page_html
typeof(page_html)
```

While the content seems unreadable now, it contains all of the information stored on the webpage and can thus be transformed into structured data. In order to do this, we first need to identify which HTML elements we are looking for on the page. This can be done using either Inspect Element (which is accessed by hovering over items and right-clicking) or by using the SelectorGadget extension on Chrome.

::: {.callout-tip title="Click To Learn More About SelectorGadget and/or Inspect Element!" collapse="true"}
As mentioned previously, SelectorGadget is an extension on Chrome that can be used to identify HTML elements. After turning on the extension, all that you need to do to find the HTML element of an item on a webpage is to point to it with your mouse and click on it. To learn more about SelectorGadget and see a demo video on how it is used, you can visit <https://selectorgadget.com/>

Another way to find HTML elements is by using Inspect Element on your browser. Here is a short tutorial video explaining how to use Inspect Element: {{< video https://youtu.be/CwiRPmXhcLY?si=BUn_AErLvHtQDmc8 aspect-ratio="21x9" >}} While this method is not as intuitive as using SelectorGadget, the Inspect Element feature is available on all browsers, whereas SelectorGadget is only available on Chrome.
:::

To scrape the article's text we need to find the HTML tag corresponding to the text. Once we identify that all of the text is encased in the `p` tag, we can display the article's text in R by running the following functions from the rvest package:

```{r}
article_text <- page_html |> 
  html_elements("p") |> 
  html_text2()
article_text
```

By comparing the text spit out by our code and the text on the website, we can see that all of the information is the same. How this works is that the function `html_elements` finds every element that starts with the tag `p` and the function `html_text2` turns the information in those tags into readable text. Following this same logic of finding HTML elements and turning them into strings in R, we can select any HTML element and store it as a string data type.

# Dataframes and Attributes

Now that we have scraped our first element and turned the HTML tags into storable information, we can now store that information in dataframes! Since our `article_text` object is a list, we must consolidate that list into a single string to work inside of a dataframe.

```{r}
article_text_combined <- paste(article_text, collapse = " ")
article_text_combined
```

If we compare the output for `article_text_combined` from this code block versus the output from the `article_text` code block above, we can see that the contents are the same. However, the `paste` function collapsed the list from `article_text` into a single string which allows us to store the article information much easier. By using the tibble package, we can create a table using the article URL and the associated text as shown below.

```{r}
library(tibble)
df_article <- tibble(
  article_url = page_url,
  article_content = article_text_combined
  )
df_article
```

Another important HTML tag that we can scrape are attributes. An attribute is an element within another element, and can contain information such as links or text. By scraping attributes, you can get the associated information within that tag. Take for example, the hyperlinks within the article text. If you hover over the links, you can see the URL displayed on the bottom left of your screen. We can create a list of all of these URL's by scraping them as shown below.

```{r}
links_in_text <- page_html |> 
  html_elements("p") |> 
  html_elements("a") |> 
  html_attr("href") 
links_in_text
```

As you may notice, the code block above looks very similar to the code block for scraping the article's text. By adding the function `html_elements('a')`, the code now filters the text for the HTML `a` tags, and the `html_attr('href')` then takes the associated links from those `a` tags. By scraping attributes, we can get the links to associated articles which can be useful for automation.

# Automation

By combining the previously learned topics of differentiating HTML elements and attributes, storing data in dataframes, and storing links that are nested in text, we may now automate the web scraping process to collect multiple pages of information at a time. While this article will only focus on automating the process on Transfermarkt, you can check the notes about strategies for automating the web scraping process on other websites. In order to automate the collection of news articles, we can navigate to the "All News" page, or use the URL <https://www.transfermarkt.com/aktuell/newsarchiv/aktuell/new/datum/2025-10-22>

(Note that the URL being used is from the day this article was written - your URL will have a different date if you choose to navigate to the page on your own or you can use the same day to follow along)

Your page should look similar to this one:

![](transfermarkt_automation.PNG){width="2496"}

Once you have navigated to the "All News" page, you should notice two things: the first being the URL structure and the second being how the articles are displayed on the page. By looking at the URL structure, we can see that the URL for every day in the news cycle follows the same format: `https://www.transfermarkt.com/aktuell/newsarchiv/aktuell/new/datum/[date]` . This means that if we change the date on the URL to a past day, we can see the corresponding news articles posted at that time. Since the only thing changing on the URL is the date at the end, we can use iterative functions to mutate the date in order to scrape articles over the course of a few days.

```{r}
#initializing dates
start_date <- as.Date("2025-01-01", format = "%Y-%m-%d")
end_date <- as.Date("2025-01-02", format = "%Y-%m-%d")

#temp value for iteration
current_date <- start_date

#creating list of dates from start to end
dates <- seq(start_date, end_date, by = "day")

#array for accessing each date in news article archive
news_archive_urls <- character()
news_archive_urls <- paste0("https://www.transfermarkt.us/aktuell/newsarchiv/aktuell/new/datum/", format(dates, "%Y-%m-%d"))

news_archive_urls
```

The code block above returns all of the URLs from January 1, 2025 to January 3, 2025. Now that we have those URLs, we can scrape the news articles on each page by finding the HTML tag corresponding to the link for each article. We can use a `for` loop to iterate through each page in the `news_archive_urls` list to collect the article URLs.

```{r}
articles_list <- character()

for (page in news_archive_urls) {
  page_html <- read_html(page)
  
  news_article <- page_html |> 
    html_elements(".newsticker__box") |> 
    html_element("a") |> 
    html_attr("href")
  
  articles_list <- append(articles_list, news_article)
  
  Sys.sleep(runif(1, 0.4, 0.8))
}

articles_list

```

After running the code block above, one thing you should notice is that every URL returned in `article_list` does not include the root of the Transfermarkt URL `https://www.transfermarkt.us/` . You will need to add this later in order for the URLS to work. Now that we have all of the news articles posted from January 1, 2025 to January 3, 2025, we can add all of their corresponding information into our previous dataframe.

```{r}
contents <- character()
full_article_urls <- character()

for (article in articles_list) {
  articles_url <- paste0("https://www.transfermarkt.us", article)
  full_article_urls <- append(full_article_urls, articles_url) 
  
  news_articles_html <- read_html(articles_url)
  
  text <- news_articles_html |> 
    html_elements("p") |> 
    html_text2()
  full_text <- paste(text, collapse = " ")
  
  contents <- append(contents, full_text)
  
  Sys.sleep(runif(1, 0.4, 0.8))
  
}
```

Now that we have the lists of article URLs and their corresponding text content, we can now create a table and append all of the information to our existing table.

```{r}
automated_articles_table <- tibble(
  article_url = full_article_urls,
  article_content = contents
)

df_article <- rbind(df_article, automated_articles_table)
df_article
```

::: {.callout-tip title="Notes About Automation" collapse="true"}
All websites are organized differently and it is possible that Transfermarkt may update and make this guide obsolete. A good way to automatically web scrape pages is to understand how URLs may be structured and indexed on other websites. Before trying to jump into automating large amounts of data from the start, it is best to test your code on small samples first.
:::

# Exercises

1.  Find the html element for the player names on the profiles along the website sidebar.

2.  Collapse the player names from the previous question into a single string and add the string to the table.

3.  Pick another article from the Transfermarkt website and add it to the table.

# Wrap-up/Conclusions

By following the steps in this module, you have now learned:

-   how to use R for web scraping

-   how to identify and extract html elements

-   how to turn scraped data into tables

-   how to automate web scraping

Although this module only focuses on articles through Transfermarkt, these tools and methods can be used on any website (with proper permissions for ethical scraping).

To learn more about HTML and web scraping, the following resources and references for this module are a great place to start:

<https://www.w3schools.com/html/html_intro.asp>

<https://www.theserverside.com/definition/HTML-Hypertext-Markup-Language>

[https://rvest.tidyverse.org/articles/rvest.html](https://rvest.tidyverse.org/articles/rvest.html#tables)

# References

<https://r4ds.hadley.nz/webscraping.html#fnref2>

<https://www.w3schools.com/html/html_intro.asp>

Elmore, R., & Urbaczewski, A. (2025). *Introduction to sports analytics using R*. Prospect Press.

# dataset

<https://github.com/janelyn-geronimo/text_mining_and_NLP/tree/main/soccer_news/scraped_articles>
